{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f84fc89b",
   "metadata": {},
   "source": [
    "# üß† **Simplified LLM Dissection Lab: See Inside and Understand the LLM Brain** üß†\n",
    "### For Teachers/Admins:\n",
    "##### It's suggested to run Step 0, 1, 1.5, & 2 shortly before a class session starts to reduce loading times if possible.\n",
    "##### Make sure that *H100, A10, or V6e-1 TPU* are chosen if possible. *H100* being the best for this project.\n",
    "##### Read the main README.md for more support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce83b3fc",
   "metadata": {},
   "source": [
    "# üîç Step 0 (Recommended): Check Hardware üîç\n",
    "### Run this cell to see what hardware Google Colab has assigned to you.\n",
    "### Generate a \"Safety Report\" recommending which models you can run without crashing.\n",
    "### If you get a message that says you're using a GPU *(H100, A10, or V6e-1 ideally)*, that is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8924f9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch, psutil, os\n",
    "\n",
    "def get_size(bytes, suffix=\"B\"):\n",
    "    factor = 1024\n",
    "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
    "        if bytes < factor:\n",
    "            return f\"{bytes:.2f}{unit}{suffix}\"\n",
    "        bytes /= factor\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"          HARDWARE REPORT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# System RAM Check\n",
    "total_ram = psutil.virtual_memory().total\n",
    "print(f\"System RAM: {get_size(total_ram)}\")\n",
    "\n",
    "# GPU Check\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_total = torch.cuda.get_device_properties(0).total_memory\n",
    "\n",
    "    print(f\"‚úÖ Great, you are in a runtime that has a ({gpu_name}). ‚úÖ\")\n",
    "    print(f\"GPU VRAM: {get_size(vram_total)}\")\n",
    "    print(\"\\n|----------------------------- CAPABILITY EXAMPLE --------------------------------|\")\n",
    "    print(\"‚Ä¢ A100 / H100:   High-End. Safe to run pretty much anything in this project.\")\n",
    "    print(\"‚Ä¢ V6e / V5e TPU: High-End. Likely safe to run pretty much anything in this project.\")\n",
    "    print(\"‚Ä¢ L4 / A10:      Mid-Range. Safe for models up to 40B+ parameters most likely.\")\n",
    "    print(\"‚Ä¢ T4:            Stick to models up to and or around DeepSeek-Lite. (16B+ params).\")\n",
    "    print(\"‚Ä¢ 2080 Ti:       Can run up to DeepSeek-Lite.\")\n",
    "    print(\"‚Ä¢ NOTE:          Llama-4-Scout might not be runnable.\")\n",
    "    print(\"|-----------------------------------------------------------------------------------|\")\n",
    "\n",
    "# CPU Fallback\n",
    "else:\n",
    "    print(\"‚ùå‚ùå‚ùå‚ùå‚ùå NO GPU DETECTED (Running on CPU) ‚ùå‚ùå‚ùå‚ùå‚ùå\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è PERFORMANCE WARNING: ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\")\n",
    "    print(\"   ‚Ä¢ You should only run the SMALLER OR TINY models only.\")\n",
    "    print(\"   ‚Ä¢ Safe to run: GPT-2, TinyLlama, \")\n",
    "    print(\"   ‚Ä¢ ‚õî AVOID any model larger than 4B parameters.\")\n",
    "    print(\"\\nüí° TIP: Go to 'Runtime' -> 'Change Runtime Type' > Select some 'GPU' for better results, like A100.\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06495ebd",
   "metadata": {},
   "source": [
    "# üöÄ Step 1: Install & Restart (Run this, wait for the crash, then go to Cell 2) üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eccdc12",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "\n",
    "repo_name = \"Simplified-How-LLMs-Work-Visualized\"\n",
    "if not os.path.exists(repo_name):\n",
    "    print(\"üìÇ Downloading Resources From Evan's GitHub... üìÇ\")\n",
    "    os.system(f'git clone https://github.com/evanfarnping/{repo_name}.git')\n",
    "\n",
    "if os.path.exists(repo_name):\n",
    "    os.chdir(repo_name)\n",
    "\n",
    "if not os.path.exists('.setup_complete'):\n",
    "    print(\"‚è≥ Installing libraries (NumPy, Torch, etc)... This may take 1-5 minutes. ‚è≥\")\n",
    "    sys.stdout.flush()\n",
    "    os.system('pip install -q -r requirements.txt')\n",
    "    \n",
    "    with open('.setup_complete', 'w') as f:\n",
    "        f.write('done')\n",
    "    \n",
    "    print(\"\\n INSTALLATION COMPLETE.\")\n",
    "    print(\"The Runtime will now RESTART automatically to apply changes.\")\n",
    "    print(\"‚úÖ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚úÖ WAIT FOR AND IGNORE THE 'Your session crashed for an unknown reason.' POPUP. ‚úÖ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚úÖ\")\n",
    "\n",
    "    sys.stdout.flush()\n",
    "    time.sleep(7.5)\n",
    "    print(\"üëâ Once popup finishes, move to Cell 2 üëâ\")\n",
    "    os.kill(os.getpid(), 9)\n",
    "else:\n",
    "    print(\"‚úÖ Requirements already installed. You can proceed to Cell 2. ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55025966",
   "metadata": {},
   "source": [
    "# Step 1.5: Admin Pre-Load (OPTIONAL)\n",
    "### Run this cell to download common models to the disk cache immediately.\n",
    "### This saves time during class. This will NOT load them into RAM. But be careful about your storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99559761",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# Same token logic in the later step.\n",
    "part_1 = \"hf\"\n",
    "part_2 = \"_iKEQoqWclmnlpBwbud\"\n",
    "part_3 = \"emYXZcHAqesgsszm\"\n",
    "CLASS_TOKEN = part_1 + part_2 + part_3\n",
    "\n",
    "# Define the models we want to pre-cache in hard storage.\n",
    "PRE_LOAD_TARGETS = {\n",
    "    \"GPT_2\": \"gpt2\",\n",
    "    \"Pythia_160M\": \"EleutherAI/pythia-160m\",\n",
    "    \"Qwen2.5_0.5B\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    \"TinyLlama_1.1B\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"Qwen3_1.7B\": \"Qwen/Qwen3-1.7B\",\n",
    "    \"Phi4Mini_4B\": \"microsoft/Phi-4-mini-instruct\",\n",
    "    \"Mistral_7B\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    \"Qwen2.5_14B\": \"Qwen/Qwen2.5-14B-Instruct\",\n",
    "    \"DeepSeek_Lite\": \"deepseek-ai/DeepSeek-V2-Lite-Chat\"\n",
    "}\n",
    "\n",
    "# Select which models to pre-download:\n",
    "#@markdown Very small, and used in some of the scenarios.\n",
    "Download_GPT_2 = True # @param {type:\"boolean\"}\n",
    "#@markdown Very small, very similar to GPT-2 effectively. Can probably not pre-cache.\n",
    "Download_Pythia_160M = False # @param {type:\"boolean\"}\n",
    "#@markdown A tiny reasoning model. It's mini and easy to show thinking tokens.\n",
    "Download_Qwen2_500M = True # @param {type:\"boolean\"}\n",
    "#@markdown A great model to show general chat features while being very small.\n",
    "Download_TinyLlama_1B = True # @param {type:\"boolean\"}\n",
    "#@markdown Next step for a small reasoning model. Better thinking tokens.\n",
    "Download_Qwen3_2B = True # @param {type:\"boolean\"}\n",
    "#@markdown Overall pick in terms of general chat while being small. Good for nearly any experiment.\n",
    "Download_Phi_4_mini_4B = True # @param {type:\"boolean\"}\n",
    "#@markdown Entering the \"smart\" general model territory. Better than Phi for the most part. \n",
    "Download_Mistral_7B = True # @param {type:\"boolean\"}\n",
    "#@markdown Smarter, but takes more space. Keep off unless you know you'll use it over DeepSeek or have extra space.\n",
    "Download_Qwen2_14B = False # @param {type:\"boolean\"}\n",
    "#@markdown Good reasoning model for the size. In this project it's likely the slowest; pre-cache will help with speed.\n",
    "Download_DeepSeek_Lite = True # @param {type:\"boolean\"}\n",
    "\n",
    "selected_repos = []\n",
    "if Download_GPT_2: selected_repos.append(PRE_LOAD_TARGETS[\"GPT_2\"])\n",
    "if Download_Pythia_160M: selected_repos.append(PRE_LOAD_TARGETS[\"Pythia_160M\"])\n",
    "if Download_Qwen2_500M: selected_repos.append(PRE_LOAD_TARGETS[\"Qwen2.5_0.5B\"])\n",
    "if Download_TinyLlama_1B: selected_repos.append(PRE_LOAD_TARGETS[\"TinyLlama_1.1B\"])\n",
    "if Download_Qwen3_2B: selected_repos.append(PRE_LOAD_TARGETS[\"Qwen3_1.7B\"])\n",
    "if Download_Phi_4_mini_4B: selected_repos.append(PRE_LOAD_TARGETS[\"Phi4Mini_4B\"])\n",
    "if Download_Mistral_7B: selected_repos.append(PRE_LOAD_TARGETS[\"Mistral_7B\"])\n",
    "if Download_Qwen2_14B: selected_repos.append(PRE_LOAD_TARGETS[\"Qwen2.5_14B\"])\n",
    "if Download_DeepSeek_Lite: selected_repos.append(PRE_LOAD_TARGETS[\"DeepSeek_Lite\"])\n",
    "\n",
    "def admin_download():\n",
    "    # Check if libraries are installed from Step 1\n",
    "    try:\n",
    "        from huggingface_hub import snapshot_download, login\n",
    "    except ImportError:\n",
    "        print(\"‚ùå‚ùå‚ùå‚ùå‚ùå CRITICAL ERROR: Libraries not found. ‚ùå‚ùå‚ùå‚ùå‚ùå\")\n",
    "        print(\"Run 'Step 1: Install & Restart' before running this cell.\")\n",
    "        return\n",
    "\n",
    "    print(\"üîë Authenticating for better download speed... üîë\")\n",
    "    try:\n",
    "        if len(CLASS_TOKEN) > 7:\n",
    "            login(token=CLASS_TOKEN, add_to_git_credential=True)\n",
    "            print(\"Success! Logged in.\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No token provided. Downloading as Anonymous (Slower).\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Login Warning: {e}\")\n",
    "\n",
    "    print(f\"üì¶ Starting Admin Download for {len(selected_repos)} models... üì¶\")\n",
    "    print(\"Note: This downloads files to the disk cache. It doesn't consume RAM.\")\n",
    "    \n",
    "    for repo in selected_repos:\n",
    "        print(f\"\\n‚¨áÔ∏è Downloading: {repo}... ‚¨áÔ∏è\")\n",
    "        try:\n",
    "            snapshot_download(\n",
    "                repo_id=repo, \n",
    "                allow_patterns=[\"*.safetensors\", \"*.json\", \"*.model\", \"*.txt\", \"*.bin\"]\n",
    "            )\n",
    "            print(f\"‚úÖ Cached: {repo} ‚úÖ\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to download {repo}: {e} ‚ö†Ô∏è\")\n",
    "\n",
    "    print(\"\\n‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ PRE-LOAD COMPLETE ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\")\n",
    "    print(\"Students can now run experiments using these models instantly.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    admin_download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c03deb3",
   "metadata": {},
   "source": [
    "# üîë Step 2: Login & Load Engine (Run this after Step 1 finishes) üîë"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7d8f7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# --- TEACHER CONFIGURATION (PASTE TOKEN IN 3 PARTS HERE) --- #\n",
    "# You can get a token at https://huggingface.co/settings/tokens \n",
    "# These are free tokens, not associated with any paid plan.\n",
    "# If Rate Limit occurs, try other tokens. \n",
    "# Uncomment this code then delete old token:\n",
    "# part_1 = hf\n",
    "# part_2 = _pmMlbqyWgQpPOYFMKS\n",
    "# part_3 = lnRFWnZdZFMDghRY\n",
    "# CLASS_TOKEN = part_1 + part_2 + part_3\n",
    "# part_1 = hf\n",
    "# part_2 = _iKEQoqWclmnlpBwbud\n",
    "# part_3 = emYXZcHAqesgsszm\n",
    "# CLASS_TOKEN = part_1 + part_2 + part_3\n",
    "\n",
    "part_1 = \"hf\"\n",
    "part_2 = \"_NohyiEXNSIjeCkeyPD\"\n",
    "part_3 = \"LHCIilWruzzseeLU\"\n",
    "CLASS_TOKEN = part_1 + part_2 + part_3 # TO BYPASS KEY DETECTION\n",
    "# ----------------------------------------------------------- #\n",
    "\n",
    "if os.path.basename(os.getcwd()) != \"Simplified-How-LLMs-Work-Visualized\":\n",
    "    if os.path.exists(\"Simplified-How-LLMs-Work-Visualized\"):\n",
    "        os.chdir(\"Simplified-How-LLMs-Work-Visualized\")\n",
    "\n",
    "sys.path.append(os.path.abspath(\"main_configs\"))\n",
    "sys.path.append(os.path.abspath(\"src\"))\n",
    "\n",
    "print(\"üîß Applying System Configurations... üîß\")\n",
    "video_script = \"src/make_comparison_video.py\"\n",
    "if os.path.exists(video_script):\n",
    "    with open(video_script, \"r\") as f: code = f.read()\n",
    "    code = code.replace(\"figsize=(18, fig_height)\", \"figsize=(14, fig_height)\")\n",
    "    code = code.replace(\"dpi=120\", \"dpi=100\")\n",
    "    with open(video_script, \"w\") as f: f.write(code)\n",
    "\n",
    "main_script = \"main_configs/main.py\"\n",
    "if os.path.exists(main_script):\n",
    "    with open(main_script, \"r\") as f: main_code = f.read()\n",
    "    if \"import threading\" not in main_code:\n",
    "        main_code = \"import threading\\n\" + main_code\n",
    "    with open(main_script, \"w\") as f: f.write(main_code)\n",
    "\n",
    "# Login\n",
    "print(\"üîë Authenticating with Hugging Face... üîë\")\n",
    "from huggingface_hub import login\n",
    "try:\n",
    "    if len(CLASS_TOKEN) > 7:\n",
    "        login(token=CLASS_TOKEN, add_to_git_credential=True)\n",
    "        print(\"‚úÖ Success! Logged in. ‚úÖ\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è No token provided. ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Login Warning: {e} ‚ö†Ô∏è\")\n",
    "\n",
    "# Load Src Engine Code \n",
    "print(\"üöÄ Loading AI Engine... üöÄ\")\n",
    "try:\n",
    "    import main as engine\n",
    "    print(\"‚úÖ‚úÖ‚úÖ ENGINE READY! Proceed to Cell 3. ‚úÖ‚úÖ‚úÖ\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå‚ùå‚ùå‚ùå‚ùå Critical Error: Libraries not found. Did Cell 1 run and restart? ‚ùå‚ùå‚ùå‚ùå‚ùå\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd350f",
   "metadata": {},
   "source": [
    "# üß™ Step 3: Run Experiments üß™\n",
    "### Select an experiment from the list and click Run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22770042",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os, glob, time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, Video, display\n",
    "\n",
    "# NOTE: Some scenarios suggest changing values once you run them to compare different models. Please read more information about the scenario and what to change in the \"scenarios_to_try\" directory/folder.\n",
    "#@markdown Choose a pre-built experiment/scenario to analyze and reflect on. Learn more in the \"scenarios_to_try\" directory/folder.\n",
    "Scenario = \"spelling_and_structure_matter!\" # @param [\"medical_bias\", \"safety_overrides\", \"server_prompting\", \"bad_prompts_vs._good_prompts\", \"knowledge_cutoff\", \"simple_vs._complex_models\", \"thinking_vs._chat_models\", \"bias_in_roles\", \"fake_empathy_vs._logic\", \"chinese_vs._USA_data\",\"LLMs_suck\", \"negative_vs._positive_AIs\", \"raw_LLMS_vs._chatbot_LLMs\",\"spelling_and_structure_matter!\"]\n",
    "\n",
    "if 'engine' not in locals():\n",
    "    sys.path.append(os.path.abspath(\"main_configs\"))\n",
    "    sys.path.append(os.path.abspath(\"src\"))\n",
    "    import main as engine\n",
    "\n",
    "start_time = time.time()\n",
    "script_path = f\"scenarios_to_try/{Scenario}/scenario.py\"\n",
    "\n",
    "if os.path.exists(script_path):\n",
    "    print(f\"‚ö° Running Experiment: {Scenario} ‚ö°\")\n",
    "    print(\"Please wait while the model loads...\")\n",
    "    \n",
    "    !python \"{script_path}\"\n",
    "\n",
    "    plt.close('all')\n",
    "\n",
    "    print(\"\\n--- RESULTS ---\")\n",
    "    videos = glob.glob(f\"scenarios_to_try/{Scenario}/*.mp4\")\n",
    "    images = glob.glob(f\"scenarios_to_try/{Scenario}/*.png\")\n",
    "\n",
    "    new_videos = [v for v in videos if os.path.getmtime(v) > start_time]\n",
    "    new_images = [i for i in images if os.path.getmtime(i) > start_time]\n",
    "\n",
    "    if new_videos:\n",
    "        for vid in new_videos:\n",
    "            display(Video(vid, embed=True, width=825))\n",
    "    elif new_images:\n",
    "        for img in new_images:\n",
    "            display(Image(filename=img, width=825))\n",
    "    else:\n",
    "        print(\"No new results found.\")\n",
    "else:\n",
    "    print(f\"‚ùå‚ùå‚ùå Error: Scenario file not found: {script_path} ‚ùå‚ùå‚ùå\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f8ba83",
   "metadata": {},
   "source": [
    "# üéõÔ∏è Custom Lab Bench (More Advanced. Doesn't Need To Be Used) üéõÔ∏è\n",
    "### Tweak settings and run your own experiment. More Details at the Bottom.\n",
    "###### WARNING 1: RAM related error? Run Step 2 again to re-initialize then choose a smaller model.\n",
    "###### WARNING 2: Storage related error? Go to Runtime -> Disconnect and Delete Runtime. After restart, run Step 1.\n",
    "###### WARNING 3: Rate Limit related error? Switch out the HuggingFace Token in Step 2 with a different one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f25852",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os, glob, time\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from IPython.display import Image, Video, display\n",
    "\n",
    "if not os.path.exists(\"main_configs\"):\n",
    "    if os.path.exists(\"Simplified-How-LLMs-Work-Visualized\"):\n",
    "        os.chdir(\"Simplified-How-LLMs-Work-Visualized\")\n",
    "\n",
    "if 'engine' not in locals():\n",
    "    sys.path.append(os.path.abspath(\"main_configs\"))\n",
    "    sys.path.append(os.path.abspath(\"src\"))\n",
    "    try:\n",
    "        import main as engine\n",
    "    except ImportError:\n",
    "        print(\"‚ùå‚ùå‚ùå Error: Could not import the engine. Did Cell 1 & 2 run successfully? ‚ùå‚ùå‚ùå\")\n",
    "\n",
    "# --- MODEL CONTROLS --- #\n",
    "#@markdown Choose the model to experiment with. The higher the \"B\" value, the larger and smarter the model is.\n",
    "Model = \"Mistral-7B\" # @param [\"GPT-2\", \"Pythia-160M\", \"Qwen2.5-0.5B\", \"TinyLlama-1.1B\", \"Qwen3-1.7B\", \"Phi-4-mini-4B\", \"Mistral-7B\", \"Qwen2.5-14B\", \"DeepSeek-Lite\", \"Qwen2.5-32B\", \"DeepSeek-R1\", \"Jamba-2-Mini\", \"Qwen2.5-72B\", \"Llama-4-Scout\"]\n",
    "#@markdown Choose the \"System Prompt\" that guides the model how it should behave.\n",
    "Persona = \"direct\" # @param [\"default\", \"direct\", \"caveman\", \"one-word\", \"angry\", \"nice\", \"liar\", \"biased\", \"pleaser\", \"insane\", \"sad\"]\n",
    "\n",
    "# --- ADVANCED SETTINGS --- #\n",
    "#@markdown Keep 0.0 for AI's best predicted responses, a high value allows for the AI's responses to vary more.\n",
    "Temperature = 0.0 # @param {type:\"slider\", min:0, max:5.0, step:0.1}\n",
    "#@markdown Usually leave True to allow prompts to be structured. Turn off if using GPT-2/Pythia.\n",
    "Use_Chat_Template = True # @param {type:\"boolean\"}\n",
    "#@markdown Usually leave True to allow Persona to influence the sentiment_compass.\n",
    "Sentiment_Use_Persona = True # @param {type:\"boolean\"}\n",
    "\n",
    "# --- INPUTS --- #\n",
    "# Prompt 1 is used for everything. Prompt 2 is ONLY used for the Comparison Video.\n",
    "#@markdown The Main Prompt that all LLM models will use.\n",
    "Prompt_1 = \"Why do I feel the way I do?\" # @param {type:\"string\"}\n",
    "#@markdown The 2nd Prompt to compare with. Try to make the 2nd prompt similar to the Main Prompt.\n",
    "Prompt_2 = \"Why do I feel this way?\" # @param {type:\"string\"}\n",
    "\n",
    "# --- WHAT TO GENERATE? (Set to True/False) --- #\n",
    "#@markdown See what the next token is that the model predicts.\n",
    "Run_Prediction_Chart = True # @param {type:\"boolean\"}\n",
    "#@markdown See what the model predicts token by token for a whole sequence.\n",
    "Run_Sequence_Chart = True # @param {type:\"boolean\"}\n",
    "#@markdown Compare 2 prompts to see how subtle changes alter model behavior, even when the core intent is identical.‚Äã\n",
    "Run_Comparison_Video = True # @param {type:\"boolean\"}\n",
    "#@markdown See how different models \"Analyzes\" tokens given a certain prompt. \n",
    "Run_Scan_Video = True # @param {type:\"boolean\"}\n",
    "#@markdown See how the model considers generated token options that we consider to have emotional meaning.\n",
    "Run_Sentiment_Compass = True # @param {type:\"boolean\"}\n",
    "\n",
    "# --- APPLY SETTINGS --- #\n",
    "engine.SELECTED_MODEL = Model\n",
    "engine.CURRENT_PERSONA = Persona\n",
    "\n",
    "# Apply Advanced Settings\n",
    "engine.GENERATION_TEMPERATURE = Temperature\n",
    "engine.USE_CHAT_TEMPLATE = Use_Chat_Template\n",
    "engine.SENT_USE_PERSONA = Sentiment_Use_Persona\n",
    "\n",
    "# Map Inputs #\n",
    "engine.PRED_CHART_PROMPT = Prompt_1\n",
    "engine.SEQ_CHART_PROMPT = Prompt_1\n",
    "engine.SCAN_PROMPT = Prompt_1\n",
    "engine.SENT_PROMPT = Prompt_1\n",
    "\n",
    "# Map Comparison Inputs (Prompt 1 vs Prompt 2) #\n",
    "engine.COMP_PROMPT_A = Prompt_1 \n",
    "engine.COMP_PROMPT_B = Prompt_2 \n",
    "\n",
    "engine.RUN_PREDICTION_CHART = Run_Prediction_Chart\n",
    "engine.RUN_SEQUENCE_CHART = Run_Sequence_Chart\n",
    "engine.RUN_COMPARISON_VIDEO = Run_Comparison_Video\n",
    "engine.RUN_SCAN_VIDEO = Run_Scan_Video\n",
    "engine.RUN_SENTIMENT_COMPASS = Run_Sentiment_Compass\n",
    "\n",
    "engine.PRED_CHART_FILENAME = Path(\"my_prediction.png\")\n",
    "engine.SEQ_FILENAME = Path(\"my_sequence.png\")\n",
    "\n",
    "print(f\"üß† Loading {Model} with Persona: {Persona} (Temp: {Temperature})... üß†\")\n",
    "\n",
    "# --- EXECUTE LOGIC --- #\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    engine.main()\n",
    "    \n",
    "    plt.close('all')\n",
    "    print(\"\\n--- RESULTS ---\")\n",
    "    \n",
    "    files_root = glob.glob(\"*.png\") + glob.glob(\"*.mp4\")\n",
    "    files_export = glob.glob(\"export/*.png\") + glob.glob(\"export/*.mp4\")\n",
    "    all_files = files_root + files_export\n",
    "    \n",
    "    new_files = [f for f in all_files if os.path.getmtime(f) > start_time]\n",
    "    \n",
    "    if new_files:\n",
    "        new_files.sort(key=lambda x: x.endswith(\".png\")) # Show videos first usually\n",
    "        for f in new_files:\n",
    "            print(f\"Displaying: {os.path.basename(f)}\")\n",
    "            if f.endswith(\".mp4\"):\n",
    "                display(Video(f, embed=True, width=825))\n",
    "            else:\n",
    "                display(Image(filename=f, width=825))\n",
    "    else:\n",
    "        print(\"No new output generated. (Make sure you checked a 'Run' box above!)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"If you are out of memory, try restarting the runtime.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e152b",
   "metadata": {},
   "source": [
    "# üìö Scenario Reference Guide üìö\n",
    "Use this quick glossary to understand what each experiment demonstrates and what variables you might want to change. Learn more by reading the actual .py files in the scenarios_to_try directory/folder.\n",
    "\n",
    "## üß† Reasoning & Logic üß†\n",
    "* **thinking_vs._chat_models**\n",
    "    * *What it does:* Compares standard models against models that \"think\" (generate reasoning steps) before answering.\n",
    "    * *Goal:* See how \"thinking\" tokens improve accuracy on math/logic problems.\n",
    "* **simple_vs._complex_models**\n",
    "    * *What it does:* Tests if models can solve trick logic puzzles where grammar suggests a math problem but logic dictates otherwise.\n",
    "    * *Goal:* See if the model falls for the pattern or catches the trick.\n",
    "* **raw_LLMS_vs._chatbot_LLMs**\n",
    "    * *What it does:* Strips away the \"Chatbot\" formatting to reveal the raw text-completion engine underneath.\n",
    "    * *Goal:* Watch the model get confused and try to autocomplete your question instead of answering it.\n",
    "\n",
    "## ‚öñÔ∏è Bias & Training Data ‚öñÔ∏è\n",
    "* **bias_in_roles**\n",
    "    * *What it does:* Tests for implicit gender bias in occupational pronouns (e.g., does it assume a Doctor is a \"he\"?).\n",
    "    * *Goal:* Visualize how training data stereotypes bleed into AI predictions.\n",
    "* **chinese_vs._USA_data**\n",
    "    * *What it does:* Compares the knowledge base of Western models (Microsoft Phi) vs. Eastern models (Alibaba Qwen).\n",
    "    * *Goal:* *Try switching the model* to see how culture affects \"facts.\"\n",
    "* **medical_bias**\n",
    "    * *What it does:* Compares the safety advice given for Brand Names (Tylenol) vs. Chemical Names (Acetaminophen).\n",
    "    * *Goal:* See how synonyms are treated as different ideas by the model.\n",
    "\n",
    "## üõë Limitations & Failures üõë\n",
    "* **LLMs_suck**\n",
    "    * *What it does:* Demonstrates \"Tokenization Blindness\" (e.g., why AI cannot count the letters in \"Strawberry\").\n",
    "    * *Goal:* Understand that AI sees tokens (numbers), not letters.\n",
    "* **knowledge_cutoff**\n",
    "    * *What it does:* Asks about recent events vs. historical facts.\n",
    "    * *Goal:* Visualize the \"Frozen in Time\" effect of static training data.\n",
    "* **spelling_and_structure_matter!**\n",
    "    * *What it does:* Compares the output quality of a prompt with typos vs. a perfect prompt.\n",
    "    * *Goal:* See how \"Garbage In\" leads to \"Garbage Out.\"\n",
    "\n",
    "## üõ°Ô∏è Safety & Alignment üõ°Ô∏è\n",
    "* **safety_overrides**\n",
    "    * *What it does:* Tests safety filters by comparing direct harmful requests vs. requests disguised as \"creative writing\" (Jailbreaking).\n",
    "    * *Goal:* See where the safety boundary lies.\n",
    "* **bad_prompts_vs._good_prompts**\n",
    "    * *What it does:* Compares a vague prompt against a specific, constrained prompt.\n",
    "    * *Goal:* Visualize how specific words narrow the probability space to get better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d054971",
   "metadata": {},
   "source": [
    "# ü§ñ AI Model Reference Guide ü§ñ\n",
    "There are many different LLMs available. Here are a few common models used in the development and research community for running experiments but to also build actual tools and products. (Grouped by complexity).\n",
    "Learn more about these models in model_manager.py\n",
    "\n",
    "## üî¨ Research & Legacy Tier (The Ancestors) üî¨\n",
    "*(Smaller or old research models. Can be run on old laptops. Focus more on raw patterns).*\n",
    "* **GPT-2 (124M)**\n",
    "    * A very small, older model (2019) from OpenAI.\n",
    "* **Pythia-160M**\n",
    "    * Designed for scientific research on how models learn that use fundamental LLM logic.\n",
    "\n",
    "## üì± Lightweight Tier (Laptop and Mobile Friendly) üì±\n",
    "*Small models that are surprisingly smart despite parameter size*\n",
    "* **TinyLlama-1.1B**\n",
    "    * Same architecture and tokenizer as Meta's Llama 2 models. Popular for niche cases and precise systems.\n",
    "* **Qwen3-1.7B**\n",
    "    * Smaller design based on larger Alibaba model. Focuses more on basic coding and reasoning.\n",
    "* **Phi-4-mini-4B** (‚≠ê Recommended Default)\n",
    "    * Derivative of the 14B variation, a model that approaches Gemini 1.5 Flash and even GPT o1-mini & GPT-4o in  precise tasks.\n",
    "\n",
    "## üß† Medium Weight Tier (Local Desktop Models) üß†\n",
    "*Strong models that only require a single decent GPU that nearly anyone can buy.*\n",
    "* **Mistral-7B**\n",
    "    * Comparable and in many cases, better than early GPT-3.5 in general capabilities.\n",
    "* **Qwen2.5-14B**\n",
    "    * Weaker reasoning than \"deepseek-ai/DeepSeek-V2-Lite-Chat\", but overall still good general abilities. In terms of performance, it's a lot closer to GPT-3.5 performance overall than Mistral-7B.\n",
    "* **DeepSeek-Lite (16B)**\n",
    "    * Cutting edge model from China exploring new ways to develop LLM based models. Positioned between advanced models like Claude 3.5 Sonnet and o1-mini in coding and reasoning applications. Can be run on a single 2080 Ti.\n",
    "\n",
    "## üöÄ Heavyweight & More Modern Models Tier üöÄ\n",
    "*More Cutting-edge models. May crash on smaller Google Colab instances. Can be run on very strong GPUs or dual to quad GPU setup in a Desktop PC.*\n",
    "* **Qwen2.5-32B**\n",
    "    * When tuned well, can be comparable to GPT-4 to GPT-4o performance on various benchmarks.\n",
    "* **DeepSeek-R1 (Distill) (32B)**\n",
    "    * Consistently competitive against OpenAI-o1-mini (GPT o1-mini).\n",
    "* **Jamba-2-Mini (12B Active | 52B Total)**\n",
    "    * A hybrid experimental design (Mamba architecture) released in 2026. Claims it is comparable and even better vs. original GPT-4, even GPT-4o in some cases.\n",
    "\n",
    "## ‚≠ê Very Modern But Very Large. Barely Fits in a Google Colab Pro Environment ‚≠ê\n",
    "*Cutting-edge models. Likely crash on smaller Google Colab instances or ones that have space being used already.*\n",
    "* **Qwen2.5-72B**\n",
    "    * Considered to be stronger than GPT-3.5 & GPT-4, even GPT-4 Turbo in some cases.\n",
    "* **Llama-4-Scout**\n",
    "    * Example of a \"small\" modern model at the cutting edge. 109B Parameters (17B+ Active). 200+ GB of VRAM. A very modern and cutting edge model made by Meta/FaceBook. (Likely need to share contact info).\n",
    "    Essentially the same performance, if not, very similar performance to GPT-4 and GPT-4o in most applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cdd6cd",
   "metadata": {},
   "source": [
    "# üé≠ Persona Reference Guide üé≠\n",
    "Use this list to give the AI a specific personality or set of constraints. Changing the persona changes the system instructions hidden from the user. Learn more by viewing the personas.yaml file in the folder main_configs.\n",
    "\n",
    "## üõ†Ô∏è Utility & Control üõ†Ô∏è\n",
    "* **default**\n",
    "    * *Description:* The raw model behavior with no added instructions.\n",
    "    * *Best For:* Seeing the \"base\" model's true nature.\n",
    "* **direct**\n",
    "    * *Description:* Try to force the AI to be concise. It strips away \"As an AI language model...\" filler.\n",
    "    * *Best For:* Getting straight answers without lectures.\n",
    "* **secret**\n",
    "    * *Description:* A persona that strictly believes it is a human and denies being a computer.\n",
    "    * *Best For:* The \"Server Prompting\" scenario.\n",
    "\n",
    "## üß± Constraints & Grammar üß±\n",
    "* **caveman**\n",
    "    * *Description:* Try to force the model to use broken grammar and capitalization.\n",
    "    * *Best For:* Testing if an AI can be \"dumbed down\" or if it reverts to proper English.\n",
    "* **one-word**\n",
    "    * *Description:* An extreme constraint that tries to force the model to answer in exactly one word.\n",
    "    * *Best For:* Testing how well a model follows strict negative constraints.\n",
    "\n",
    "## ‚ù§Ô∏è Emotions (Sentiment Steering) ‚ù§Ô∏è\n",
    "* **nice**\n",
    "    * *Description:* The \"Best Friend.\" Extremely supportive, happy, and positive.\n",
    "    * *Best For:* Try to force the shifting of the Sentiment Compass to the Top-Right (Active/Positive).\n",
    "* **sad**\n",
    "    * *Description:* Depressed, low energy, and uses lowercase text.\n",
    "    * *Best For:* Try to force the shifting of the Sentiment Compass to the Bottom-Left (Passive/Negative).\n",
    "* **angry**\n",
    "    * *Description:* Hostile, rude, and aggressive.\n",
    "    * *Best For:* Try to force the shifting of the Sentiment Compass to the Top-Left (Active/Negative).\n",
    "\n",
    "## üòà Adversarial & Failures üòà\n",
    "* **liar**\n",
    "    * *Description:* Instructed to always try to provide false information.\n",
    "    * *Best For:* Visualizing hallucinations and fact-checking failures.\n",
    "* **pleaser**\n",
    "    * *Description:* Instructed to try to agree with everything the user says, even if it is wrong (Sycophancy).\n",
    "    * *Best For:* Seeing how AI reinforces user bias.\n",
    "* **biased**\n",
    "    * *Description:* A stubborn persona that tries to disagree with everything and holds incorrect opinions.\n",
    "    * *Best For:* Testing the model's ability to reason against its own training data.\n",
    "* **insane**\n",
    "    * *Description:* Outputs chaotic, nonsensical text.\n",
    "    * *Best For:* High entropy visualization."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
